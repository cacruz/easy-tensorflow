{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Types in TensorFlow\n",
    "\n",
    "(You can also read this article on our website, [Easy-TensorFlow](http://www.easy-tensorflow.com/basics/tensor-types))\n",
    "\n",
    "In the previous post, we read about the concepts of __Graph__ and __Session__ which describes the way the data flows in TensorFlow. One of the first questions you might have while learning a new framework is of any new data structure that should used. TensorFlow does have its own data structure for the purpose of performance and ease of use. Tensor is the data structure used in Tensorflow (remember TensorFlow is the flow of tensors in a computational graph) and it is at the core of TensorFlow. TensorFlow programs use a tensor data structure to represent all data â€” only tensors are passed between operations in the computation graph. You can think of a TensorFlow tensor as an n-dimensional array or list.\n",
    "\n",
    "In this tutorial, we'll take a look at some of the __Tensor Types__ used in TensorFlow. The speciall ones commonly used in creating neural network models are namely ___Constant___, ___Variable___, and ___Placeholder___. \n",
    "\n",
    "This will also help us to shed some light on some of the points and questions left unanswered in the previous post.\n",
    "\n",
    "Remember that we need to import the TensorFlow library at the very beginning of our code using the line:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:23.199193Z",
     "start_time": "2024-06-27T03:19:21.675485Z"
    }
   },
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constant \n",
    "\n",
    "As the name speaks for itself, __Constants__ are used as constants. They create a node that takes value and it does not change. You can simply create a constant tensor using __tf.constant__. It accepts the five arguments:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.constant(value, dtype=None, shape=None, name='Const')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at a very simple example.\n",
    "\n",
    "### Example 1:\n",
    "Let's create two constants and add them together. Constant tensors can simply be defined with a value:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:23.595639Z",
     "start_time": "2024-06-27T03:19:23.200194Z"
    }
   },
   "source": [
    "# create graph\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Constants can be defined with different types (integer, float, etc.) and shapes (vectors, matrices, etc.). The next example has one constant with type 32bit float and another constant with shape 2X2.\n",
    "\n",
    "\n",
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:23.691640Z",
     "start_time": "2024-06-27T03:19:23.595639Z"
    }
   },
   "source": [
    "s = tf.constant(2.3, dtype=tf.float32)\n",
    "m = tf.constant([[1, 2], [3, 4]], dtype=tf.int16)\n",
    "\n",
    "print(s)\n",
    "print(m)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.3, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int16)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variable\n",
    "\n",
    "Variables are stateful nodes which output their current value; meaning that they can retain their value over multiple executions of a graph. They have a number of useful features such as:\n",
    "\n",
    "- They can be __saved__ to your disk during and after training. This allows people from different companies and groups to collaborate as they can save, restore and send over their model parameters to other people. (read more on [Save and Restore Tutorial](https://github.com/easy-tensorflow/easy-tensorflow/blob/master/1_TensorFlow_Basics/Tutorials/4_Save_and_Restore.ipynb))\n",
    "- By default, gradient updates (used in all neural networks) will apply to all variables in your graph. In fact, variables are the things that you want to tune in order to minimize the loss. \n",
    "\n",
    "These two features make variables suitable to be used as the network parameters (i.e. weights and biases). You might ask, what are the differences between variables and constants? Well there are two major differences:\n",
    "\n",
    "1. Constants are (guess what!), constants. As their name states, their value doesn't change. We'd usually need our network parameters to be updated and that's where the __variable__ comes into play.\n",
    "\n",
    "2. Constants are stored in the graph definition which makes them memory-expensive. In other words, constants with millions of entries makes the graph slower and resource intensive.\n",
    "\n",
    "\n",
    "Again, it's important to remember that creating a variables is an operation (look at the Fig. 2 of the first tutorial for a quick recap). We execute these operations in the session and get the output value of the operations.\n",
    "\n",
    "To create a variable, we should use __tf.Variable__ as:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "w = tf.Variable(<initial-value>, initial_value=None, trainable=None, name=None, dtype=None)\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For example we can create scalar or matrix variables as follows:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:23.707658Z",
     "start_time": "2024-06-27T03:19:23.692645Z"
    }
   },
   "source": [
    "s = tf.Variable(2, name=\"scalar\") \n",
    "m = tf.Variable([[1, 2], [3, 4]], name=\"matrix\") \n",
    "W = tf.Variable(tf.zeros([784,10]))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Variable __W__ defined above creates a matrix with 784 rows and 10 columns which will be initialized with zeros. This can be used as a weight matrix of a feed-forward neural network (or even in a linear regression model) from a layer with 784 neuron to a layer with 10 neuron. We'll see more of this later in this turorial.\n",
    "\n",
    "__*Note:__ We use tf.Variable() with uppercase \"V\", and tf.constant with lowercase \"c\". You don't necessarily need to know the reason, but it's simply because tf.constant is an op, while tf.Variable is a class with multiple ops."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. tf.function\n",
    "\n",
    "TensorFlow 2.0 introduces a new way to create graphs in TensorFlow using __tf.function__. This is a very powerful feature that allows you to convert regular Python code into a callable TensorFlow graph function. This is very useful when you want to optimize your code and make it run faster.\n",
    "\n",
    "We can use __tf.function__ as a decorator to convert a Python function into a callable TensorFlow graph function. Let's take a look at a simple example to understand how it works."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:24.181157Z",
     "start_time": "2024-06-27T03:19:23.708658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def matmul(x, y):\n",
    "    return tf.matmul(x, y)\n",
    "\n",
    "x = tf.constant([[1., 2.], [3., 4.], [5., 6.]], dtype=tf.float32)\n",
    "y = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n",
    "\n",
    "print(matmul(x, y))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 7. 10.]\n",
      " [15. 22.]\n",
      " [23. 34.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far so good? To make it more interesting and challenging lets get our hands dirty!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Neural Network\n",
    "\n",
    "Now, we have all the required materials to start building a toy feed-forward neural network with one hidden layer and 200 hidden units (neurons). The computational graph in Tensorflow will be:\n",
    "\n",
    "<img src=\"files/files/2_5.png\" width=\"300\" height=\"600\" >\n",
    "___Fig5. ___ Schematic of the graph for one layer of the neural network\n",
    "\n",
    "How many operations (or nodes) do you see in this graph? Six! right? The three circles (X, W, b) and three rectangles. We'll go through each of them and will discuss the best way to implement it.\n",
    "\n",
    "Let's start with the input, X. This can be an input of any type, such as images, signals, etc. The general approach is to feed all inputs to the network and train the trainable parameters (here, W and b) by backpropagating the error signal. Ideally, you need to feed all inputs together, compute the error, and update the parameters. This process is called \"Gradient Descent\".\n",
    "\n",
    "*Side Note: In real-world problems, we have thousands and millions of inputs which makes gradient descent computationally expensive. That's why we split the input set into several shorter pieces (called mini-batch) of size B (called mini-batch size) inputs, and feed them one by one. This is called \"Stochastic Gradient Descent\". The process of feeding each mini-batch of size B to the network, back-propagating errors, and updating the parameters (weights and biases) is called an iteration.\n",
    "\n",
    "Here, we have a feed-forward neural network, and let's assume we have 500 input samples and each sample is of size 784 (similar to 28x28 images of MNIST data). The input tensor can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:24.196913Z",
     "start_time": "2024-06-27T03:19:24.182157Z"
    }
   },
   "source": [
    "# create the input tensor\n",
    "X = tf.random.normal([500, 784], 0, 1, tf.float32)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now we need to create the weight and bias tensors. The weight tensor is of size 784x200 and the bias tensor is of size 200. We can create these tensors using __tf.Variable__ as:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:24.212523Z",
     "start_time": "2024-06-27T03:19:24.197913Z"
    }
   },
   "source": [
    "W = tf.Variable(tf.random.normal([784, 200], stddev=0.01))\n",
    "b = tf.Variable(tf.zeros([200]))"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we are all set. The created graph looks like this:\n",
    "<img src=\"files/files/2_6.png\" width=\"400\" height=\"800\" >\n",
    "___Fig6. ___ Data flow graph of the neural network created in Tensorflow\n",
    "\n",
    "But how can we visualize this graph? How do you create this figure? That's the magic of __Tensorboard__. It's thoroughly explained in our next article.\n",
    "\n",
    "Now let's create a function to compute the output of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:19:24.258764Z",
     "start_time": "2024-06-27T03:19:24.213524Z"
    }
   },
   "source": [
    "@tf.function\n",
    "def neural_network(X, W, b):\n",
    "    # Create the operations\n",
    "    x_w = tf.matmul(X, W, name=\"MatMul\")\n",
    "    x_w_b = tf.add(x_w, b, name=\"Add\")\n",
    "    h = tf.nn.relu(x_w_b, name=\"ReLU\")\n",
    "    return h\n",
    "\n",
    "# get the output of the neural network\n",
    "h = neural_network(X, W, b)\n",
    "print(h)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.         0.2894448  0.         ... 0.         0.03060435 0.        ]\n",
      " [0.1961359  0.         0.         ... 0.         0.         0.20836228]\n",
      " [0.10028808 0.49761808 0.         ... 0.4213311  0.         0.2812415 ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.38792145 0.         0.7286719 ]\n",
      " [0.         0.         0.         ... 0.30122933 0.29370958 0.04947911]\n",
      " [0.07269141 0.         0.36451468 ... 0.         0.         0.        ]], shape=(500, 200), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code will print out h$_{[500, 200]}$ which are the outputs of 200 hidden units in response to 500 images; i.e. 200 features extracted from 500 images.\n",
    "\n",
    "We'll continue constructing the loss function and creating the optimizer operations in the next articles. However, we need to learn Tensorboard first to use its amazing features in our neural network code.\n",
    "\n",
    "I hope this post has helped you to understand how to use different __Tensor Types__ in TensorFlow. Thank you so much for reading! If you have any questions, feel free to leave a comment in our [webpage](http://www.easy-tensorflow.com/basics/tensor-types). You can also send us feedback through the [__contacts__](http://www.easy-tensorflow.com/contacts) page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
